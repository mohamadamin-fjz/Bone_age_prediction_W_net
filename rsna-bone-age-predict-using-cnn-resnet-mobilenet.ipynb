{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":15122,"sourceType":"datasetVersion","datasetId":10832},{"sourceId":8540415,"sourceType":"datasetVersion","datasetId":5102041},{"sourceId":8648546,"sourceType":"datasetVersion","datasetId":5180270}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n\n        print(os.path.join(dirname, filename))\n        break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-09T19:36:29.490117Z","iopub.execute_input":"2024-06-09T19:36:29.490461Z","iopub.status.idle":"2024-06-09T19:36:47.139398Z","shell.execute_reply.started":"2024-06-09T19:36:29.490435Z","shell.execute_reply":"2024-06-09T19:36:47.138393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nfrom torch import Tensor\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.ndimage import grey_opening\nimport tqdm\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split \n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:22:13.571367Z","iopub.execute_input":"2024-06-09T20:22:13.571704Z","iopub.status.idle":"2024-06-09T20:22:17.736884Z","shell.execute_reply.started":"2024-06-09T20:22:13.571662Z","shell.execute_reply":"2024-06-09T20:22:17.735809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, in_filters, out_filters, seperable=True):\n        super(Block, self).__init__()\n        \n        if seperable:\n            \n            self.spatial1=nn.Conv2d(in_filters, in_filters, kernel_size=3, groups=in_filters, padding=1)\n            self.depth1=nn.Conv2d(in_filters, out_filters, kernel_size=1)\n            \n            self.conv1=lambda x: self.depth1(self.spatial1(x))\n            \n            self.spatial2=nn.Conv2d(out_filters, out_filters, kernel_size=3, padding=1, groups=out_filters)\n            self.depth2=nn.Conv2d(out_filters, out_filters, kernel_size=1)\n            \n            self.conv2=lambda x: self.depth2(self.spatial2(x))\n            \n        else:\n            \n            self.conv1=nn.Conv2d(in_filters, out_filters, kernel_size=3, padding=1)\n            self.conv2=nn.Conv2d(out_filters, out_filters, kernel_size=3, padding=1)\n        \n        self.batchnorm1=nn.BatchNorm2d(out_filters)\n        self.batchnorm2=nn.BatchNorm2d(out_filters)\n\n    def forward(self, x):\n        \n        x=self.batchnorm1(self.conv1(x)).clamp(0)\n        \n        x=self.batchnorm2(self.conv2(x)).clamp(0)\n        \n        return x\n\nclass UEnc(nn.Module):\n    def __init__(self, squeeze, ch_mul=64, in_chans=3):\n        super(UEnc, self).__init__()\n        \n        self.enc1=Block(in_chans, ch_mul, seperable=False)\n        self.enc2=Block(ch_mul, 2*ch_mul)\n        self.enc3=Block(2*ch_mul, 4*ch_mul)\n        self.enc4=Block(4*ch_mul, 8*ch_mul)\n        \n        self.middle=Block(8*ch_mul, 16*ch_mul)\n        \n        self.up1=nn.ConvTranspose2d(16*ch_mul, 8*ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.dec1=Block(16*ch_mul, 8*ch_mul)\n        self.up2=nn.ConvTranspose2d(8*ch_mul, 4*ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.dec2=Block(8*ch_mul, 4*ch_mul)\n        self.up3=nn.ConvTranspose2d(4*ch_mul, 2*ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.dec3=Block(4*ch_mul, 2*ch_mul)\n        self.up4=nn.ConvTranspose2d(2*ch_mul, ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.dec4=Block(2*ch_mul, ch_mul, seperable=False)\n        \n        self.final=nn.Conv2d(ch_mul, squeeze, kernel_size=(1, 1))\n        \n    def forward(self, x):\n        \n        enc1=self.enc1(x)\n        \n        enc2=self.enc2(F.max_pool2d(enc1, (2, 2)))\n        \n        enc3=self.enc3(F.max_pool2d(enc2, (2,2)))\n        \n        enc4=self.enc4(F.max_pool2d(enc3, (2,2)))\n        \n        \n        middle=self.middle(F.max_pool2d(enc4, (2,2)))\n        \n        \n        up1=torch.cat([enc4, self.up1(middle)], 1)\n        dec1=self.dec1(up1)\n        \n        up2=torch.cat([enc3, self.up2(dec1)], 1)\n        dec2=self.dec2(up2)\n        \n        up3=torch.cat([enc2, self.up3(dec2)], 1)\n        dec3=self.dec3(up3)\n        \n        up4=torch.cat([enc1, self.up4(dec3)], 1)\n        dec4=self.dec4(up4)\n        \n        \n        final=self.final(dec4)\n        \n        return final\n    def middle_forward(self,x):\n        enc1=self.enc1(x)\n        \n        enc2=self.enc2(F.max_pool2d(enc1, (2, 2)))\n        \n        enc3=self.enc3(F.max_pool2d(enc2, (2,2)))\n        \n        enc4=self.enc4(F.max_pool2d(enc3, (2,2)))\n        \n        \n        middle=self.middle(F.max_pool2d(enc4, (2,2)))\n        \n        return middle\n        \n\nclass UDec(nn.Module):\n    def __init__(self, squeeze, ch_mul=64, in_chans=3):\n        super(UDec, self).__init__()\n        \n        self.enc1=Block(squeeze, ch_mul, seperable=False)\n        self.enc2=Block(ch_mul, 2*ch_mul)\n        self.enc3=Block(2*ch_mul, 4*ch_mul)\n        self.enc4=Block(4*ch_mul, 8*ch_mul)\n        \n        self.middle=Block(8*ch_mul, 16*ch_mul)\n        \n        self.up1=nn.ConvTranspose2d(16*ch_mul, 8*ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.dec1=Block(16*ch_mul, 8*ch_mul)\n        self.up2=nn.ConvTranspose2d(8*ch_mul, 4*ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.dec2=Block(8*ch_mul, 4*ch_mul)\n        self.up3=nn.ConvTranspose2d(4*ch_mul, 2*ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.dec3=Block(4*ch_mul, 2*ch_mul)\n        self.up4=nn.ConvTranspose2d(2*ch_mul, ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.dec4=Block(2*ch_mul, ch_mul, seperable=False)\n        \n        self.final=nn.Conv2d(ch_mul, in_chans, kernel_size=(1, 1))\n        \n    def forward(self, x):\n        \n        enc1 = self.enc1(x)\n        \n        enc2 = self.enc2(F.max_pool2d(enc1, (2, 2)))\n        \n        enc3 = self.enc3(F.max_pool2d(enc2, (2,2)))\n        \n        enc4 = self.enc4(F.max_pool2d(enc3, (2,2)))\n        \n        \n        middle = self.middle(F.max_pool2d(enc4, (2,2)))\n        \n        \n        up1 = torch.cat([enc4, self.up1(middle)], 1)\n        dec1 = self.dec1(up1)\n        \n        up2 = torch.cat([enc3, self.up2(dec1)], 1)\n        dec2 = self.dec2(up2)\n        \n        up3 = torch.cat([enc2, self.up3(dec2)], 1)\n        dec3 =self.dec3(up3)\n        \n        up4 = torch.cat([enc1, self.up4(dec3)], 1)\n        dec4 = self.dec4(up4)\n        \n        \n        final=self.final(dec4)\n        \n        return final\n\nclass WNet(nn.Module):\n    def __init__(self, squeeze, ch_mul=64, in_chans=3, out_chans=1000):\n        super(WNet, self).__init__()\n        if out_chans==1000:\n            out_chans=in_chans\n        self.UEnc=UEnc(squeeze, ch_mul, in_chans)\n        self.UDec=UDec(squeeze, ch_mul, out_chans)\n    def forward(self, x, returns='both'):\n        \n        enc = self.UEnc(x)\n        \n        if returns=='enc':\n            return enc\n        \n        dec=self.UDec(F.softmax(enc, 1))\n        \n        if returns=='dec':\n            return dec\n        \n        if returns=='both':\n            return enc, dec\n        \n        else:\n            raise ValueError('Invalid returns, returns must be in [enc dec both]')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:22:17.738492Z","iopub.execute_input":"2024-06-09T20:22:17.738933Z","iopub.status.idle":"2024-06-09T20:22:17.776901Z","shell.execute_reply.started":"2024-06-09T20:22:17.738905Z","shell.execute_reply":"2024-06-09T20:22:17.775458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DataLoader implementation\nclass ImageDataset(torch.utils.data.Dataset):\n    def __init__(self, image_dir, transform=None):\n        self.image_dir = image_dir\n        self.transform = transform\n        self.image_paths = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith(('.png', '.jpg', '.jpeg'))][:1000]\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image\n\nclass SelfNormalize(object):\n    def __call__(self, img):\n        img_tensor = transforms.functional.to_tensor(img)\n        mean = img_tensor.mean(dim=(1, 2), keepdim=True)\n        std = img_tensor.std(dim=(1, 2), keepdim=True) + 1e-6\n        img_tensor = (img_tensor - mean) / std\n        return img_tensor\n\n# Define the transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    SelfNormalize()\n])\n\n# Create the dataset\nimage_dir = \"/kaggle/input/rsna-bone-age/boneage-training-dataset/boneage-training-dataset/\"\ndataset = ImageDataset(image_dir=image_dir, transform=transform)\n\n# Create the DataLoader\ntrain_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\n# DataLoader implementation\nclass ImageDataset(torch.utils.data.Dataset):\n    def __init__(self, image_dir, transform=None):\n        self.image_dir = image_dir\n        self.transform = transform\n        self.image_paths = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith(('.png', '.jpg', '.jpeg'))][1000:1300]\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image\n\nclass SelfNormalize(object):\n    def __call__(self, img):\n        img_tensor = transforms.functional.to_tensor(img)\n        mean = img_tensor.mean(dim=(1, 2), keepdim=True)\n        std = img_tensor.std(dim=(1, 2), keepdim=True) + 1e-6\n        img_tensor = (img_tensor - mean) / std\n        return img_tensor\n\n# Define the transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    SelfNormalize()\n])\n\n# Create the dataset\nimage_dir = \"/kaggle/input/rsna-bone-age/boneage-training-dataset/boneage-training-dataset/\"\ndataset = ImageDataset(image_dir=image_dir, transform=transform)\n\n# Create the DataLoader\ntest_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:22:17.779556Z","iopub.execute_input":"2024-06-09T20:22:17.779917Z","iopub.status.idle":"2024-06-09T20:22:18.166392Z","shell.execute_reply.started":"2024-06-09T20:22:17.779886Z","shell.execute_reply":"2024-06-09T20:22:18.165550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gaussian_kernel(radius: int = 3, sigma: float = 4, device='cpu'):\n    x_2 = np.linspace(-radius, radius, 2*radius+1) ** 2\n    dist = np.sqrt(x_2.reshape(-1, 1) + x_2.reshape(1, -1)) / sigma\n    kernel = norm.pdf(dist) / norm.pdf(0)\n    kernel = torch.from_numpy(kernel.astype(np.float32))\n    kernel = kernel.view((1, 1, kernel.shape[0], kernel.shape[1]))\n\n    if device == 'cuda':\n        kernel = kernel.cuda()\n\n    return kernel\n\n\nclass NCutLoss2D(nn.Module):\n    r\"\"\"Implementation of the continuous N-Cut loss, as in:\n    'W-Net: A Deep Model for Fully Unsupervised Image Segmentation', by Xia, Kulis (2017)\"\"\"\n\n    def __init__(self, radius: int = 4, sigma_1: float = 5, sigma_2: float = 1):\n        r\"\"\"\n        :param radius: Radius of the spatial interaction term\n        :param sigma_1: Standard deviation of the spatial Gaussian interaction\n        :param sigma_2: Standard deviation of the pixel value Gaussian interaction\n        \"\"\"\n        super(NCutLoss2D, self).__init__()\n        self.radius = radius\n        self.sigma_1 = sigma_1  # Spatial standard deviation\n        self.sigma_2 = sigma_2  # Pixel value standard deviation\n\n    def forward(self, labels: Tensor, inputs: Tensor) -> Tensor:\n        r\"\"\"Computes the continuous N-Cut loss, given a set of class probabilities (labels) and raw images (inputs).\n        Small modifications have been made here for efficiency -- specifically, we compute the pixel-wise weights\n        relative to the class-wide average, rather than for every individual pixel.\n\n        :param labels: Predicted class probabilities\n        :param inputs: Raw images\n        :return: Continuous N-Cut loss\n        \"\"\"\n        num_classes = labels.shape[1]\n        kernel = gaussian_kernel(radius=self.radius, sigma=self.sigma_1, device=labels.device.type)\n        loss = 0\n\n        for k in range(num_classes):\n            # Compute the average pixel value for this class, and the difference from each pixel\n            class_probs = labels[:, k].unsqueeze(1)\n            class_mean = torch.mean(inputs * class_probs, dim=(2, 3), keepdim=True) / \\\n                torch.add(torch.mean(class_probs, dim=(2, 3), keepdim=True), 1e-5)\n            diff = (inputs - class_mean).pow(2).sum(dim=1).unsqueeze(1)\n\n            # Weight the loss by the difference from the class average.\n            weights = torch.exp(diff.pow(2).mul(-1 / self.sigma_2 ** 2))\n\n            # Compute N-cut loss, using the computed weights matrix, and a Gaussian spatial filter\n            numerator = torch.sum(class_probs * F.conv2d(class_probs * weights, kernel, padding=self.radius))\n            denominator = torch.sum(class_probs * F.conv2d(weights, kernel, padding=self.radius))\n            loss += nn.L1Loss()(numerator / torch.add(denominator, 1e-6), torch.zeros_like(numerator))\n\n        return num_classes - loss\n\n\nclass OpeningLoss2D(nn.Module):\n    r\"\"\"Computes the Mean Squared Error between computed class probabilities their grey opening.  Grey opening is a\n    morphology operation, which performs an erosion followed by dilation.  Conceptually, this encourages the network\n    to return sharper boundaries to objects in the class probabilities.\n\n    NOTE:  Original loss term -- not derived from the paper for NCutLoss2D.\"\"\"\n\n    def __init__(self, radius: int = 2):\n        r\"\"\"\n        :param radius: Radius for the channel-wise grey opening operation\n        \"\"\"\n        super(OpeningLoss2D, self).__init__()\n        self.radius = radius\n\n    def forward(self, labels: Tensor, *args) -> Tensor:\n        r\"\"\"Computes the Opening loss -- i.e. the MSE due to performing a greyscale opening operation.\n\n        :param labels: Predicted class probabilities\n        :param args: Extra inputs, in case user also provides input/output image values.\n        :return: Opening loss\n        \"\"\"\n        smooth_labels = labels.clone().detach().cpu().numpy()\n        for i in range(labels.shape[0]):\n            for j in range(labels.shape[1]):\n                smooth_labels[i, j] = grey_opening(smooth_labels[i, j], self.radius)\n\n        smooth_labels = torch.from_numpy(smooth_labels.astype(np.float32))\n        if labels.device.type == 'cuda':\n            smooth_labels = smooth_labels.cuda()\n\n        return nn.MSELoss()(labels, smooth_labels.detach())","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:22:18.168037Z","iopub.execute_input":"2024-06-09T20:22:18.168360Z","iopub.status.idle":"2024-06-09T20:22:18.186384Z","shell.execute_reply.started":"2024-06-09T20:22:18.168333Z","shell.execute_reply":"2024-06-09T20:22:18.185570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Assuming WNet and loss function are defined elsewhere\n\n# Define the W-Net model\nmodel = WNet(5)\npsi = 0.5\nloss=NCutLoss2D()\n# Use DataParallel for multi-GPU support\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Training setup\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 20\nepoch_rec_losses_train = []\nepoch_ncut_losses_train = []\nepoch_rec_losses_test = []\nepoch_ncut_losses_test = []\n\nprint('Starting training...')\nfor epoch in tqdm.tqdm(range(num_epochs)):\n    model.train()\n    rec_losses_train = 0.0\n    n_cut_losses_train = 0.0\n\n    for images in train_loader:\n        images = images.to(device)\n        enc = model(images, returns='enc')\n        n_cut_loss = loss(enc, images)\n        n_cut_loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        dec = model(images, returns='dec')\n        rec_loss = criterion(dec, images)\n        rec_loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        rec_losses_train += rec_loss.item()\n        n_cut_losses_train += n_cut_loss.item()\n\n    epoch_rec_loss_train = rec_losses_train / len(train_loader.dataset)\n    epoch_ncut_loss_train = n_cut_losses_train / len(train_loader.dataset)\n    epoch_rec_losses_train.append(epoch_rec_loss_train)\n    epoch_ncut_losses_train.append(epoch_ncut_loss_train)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train rec Loss: {epoch_rec_loss_train:.4f}\")\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train ncut Loss: {epoch_ncut_loss_train:.4f}\")\n\n    # Validation loop\n    model.eval()\n    rec_losses_test = 0.0\n    n_cut_losses_test = 0.0\n\n    with torch.no_grad():\n        for images in test_loader:\n            images = images.to(device)\n            enc = model(images, returns='enc')\n            n_cut_loss = loss(enc, images)\n            dec = model(images, returns='dec')\n            rec_loss = criterion(dec, images)\n            rec_losses_test += rec_loss.item()\n            n_cut_losses_test += n_cut_loss.item()\n\n    epoch_rec_loss_test = rec_losses_test / len(test_loader.dataset)\n    epoch_ncut_loss_test = n_cut_losses_test / len(test_loader.dataset)\n    epoch_rec_losses_test.append(epoch_rec_loss_test)\n    epoch_ncut_losses_test.append(epoch_ncut_loss_test)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Test rec Loss: {epoch_rec_loss_test:.4f}\")\n    print(f\"Epoch {epoch+1}/{num_epochs}, Test ncut Loss: {epoch_ncut_loss_test:.4f}\")\n\n    # Save the model every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        model_path = f'/kaggle/working/model_epoch_{epoch+1}.pth'\n        torch.save(model.state_dict(), model_path)\n        print(f'Model saved at {model_path}')\n\nprint(\"Training complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:22:18.187399Z","iopub.execute_input":"2024-06-09T20:22:18.187659Z","iopub.status.idle":"2024-06-09T20:22:21.323232Z","shell.execute_reply.started":"2024-06-09T20:22:18.187636Z","shell.execute_reply":"2024-06-09T20:22:21.321671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nplt.plot(epoch_ncut_losses_test)\nplt.show()\nplt.plot(epoch_rec_losses_test)\nplt.show()\nplt.plot(epoch_ncut_losses_train)\nplt.show()\nplt.plot(epoch_rec_losses_train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:35:25.779013Z","iopub.execute_input":"2024-06-07T08:35:25.779814Z","iopub.status.idle":"2024-06-07T08:35:26.749645Z","shell.execute_reply.started":"2024-06-07T08:35:25.779782Z","shell.execute_reply":"2024-06-07T08:35:26.748804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model file from the Kaggle dataset\nmodel_path = '/kaggle/input/w-net-pretrained/model_epoch_20.pth'  # Update the path to your dataset\n\n# Initialize the model\nmodel = WNet(5)  # Update the values as needed\n# Load the state dict\nmodel.load_state_dict(torch.load(model_path))\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:22:29.733661Z","iopub.execute_input":"2024-06-09T20:22:29.734045Z","iopub.status.idle":"2024-06-09T20:22:30.725820Z","shell.execute_reply.started":"2024-06-09T20:22:29.734015Z","shell.execute_reply":"2024-06-09T20:22:30.724725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nclass AgePredictor(nn.Module):\n    def __init__(self, uenc, latent_dim, hidden_dims, output_dim):\n        super(AgePredictor, self).__init__()\n        self.uenc = uenc\n        self.fc_layers = nn.ModuleList()\n        \n        # Update input_dim to include the gender feature\n        input_dim = latent_dim + 1  # Adding 1 for the gender feature\n        \n        # Create fully connected layers\n        prev_dim = input_dim\n        for hidden_dim in hidden_dims:\n            self.fc_layers.append(nn.Linear(prev_dim, hidden_dim))\n            prev_dim = hidden_dim\n        \n        # Final layer to predict age\n        self.fc_layers.append(nn.Linear(prev_dim, output_dim))\n        \n    def forward(self, x, gender):\n        # Get the latent space from UEnc's forward_middle method\n        latent = self.uenc.middle_forward(x)\n        \n        # Flatten the latent space\n        latent = latent.view(latent.size(0), -1)\n        \n        # Concatenate the gender feature\n        latent = torch.cat((latent, gender.unsqueeze(1)), dim=1)\n        \n        # Pass through fully connected layers\n        for layer in self.fc_layers[:-1]:\n            latent = F.relu(layer(latent))\n        \n        # Final output layer\n        age = self.fc_layers[-1](latent)\n        \n        return age\n# Assuming UEnc is already defined\nuenc = model.UEnc\ninput_dim = 16 * 64 * (224 // 16) * (224 // 16)  # Adjust based on the actual input dimensions\nhidden_dims = [512, 256, 128, 64, 32, 16]\noutput_dim = 1\n\nage_predictor = AgePredictor(uenc, input_dim, hidden_dims, output_dim)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:22:34.371610Z","iopub.execute_input":"2024-06-09T20:22:34.371958Z","iopub.status.idle":"2024-06-09T20:22:35.220919Z","shell.execute_reply.started":"2024-06-09T20:22:34.371930Z","shell.execute_reply":"2024-06-09T20:22:35.219993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the CSV file containing image paths and corresponding ages\nimport pandas as pd\nbone_age_train_data = pd.read_csv(\"/kaggle/input/rsna-bone-age/boneage-training-dataset.csv\")\nbone_age_train_data['Image_Path'] = bone_age_train_data['id'].astype(str) + \".png\"\ndef to_int(x):\n    return int(x)\nbone_age_train_data['male']=bone_age_train_data['male'].apply(to_int)\n\n# Split the dataset into training and cross-validation sets\ntrain_df, val_df = train_test_split(bone_age_train_data[:2000], test_size=0.3, random_state=42)\n\n# DataLoader implementation\nclass ImageDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, image_dir, transform=None):\n        self.dataframe = dataframe\n        self.image_dir = image_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        # Get the image path, age, and gender from the dataframe\n        img_name = self.dataframe.iloc[idx]['Image_Path']\n        age = self.dataframe.iloc[idx]['boneage']\n        gender = self.dataframe.iloc[idx]['male']  # Assuming 'male' is the column name for gender (0 or 1)\n        \n        # Load and transform the image\n        img_path = os.path.join(self.image_dir, img_name)\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        \n        return image, age, gender\n\nclass SelfNormalize(object):\n    def __call__(self, img):\n        img_tensor = transforms.functional.to_tensor(img)\n        mean = img_tensor.mean(dim=(1, 2), keepdim=True)\n        std = img_tensor.std(dim=(1, 2), keepdim=True) + 1e-6\n        img_tensor = (img_tensor - mean) / std\n        return img_tensor\n\n# Define the transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    SelfNormalize()\n])\n\n# Create the datasets\nimage_dir = \"/kaggle/input/rsna-bone-age/boneage-training-dataset/boneage-training-dataset/\"\ntrain_dataset = ImageDataset(dataframe=train_df, image_dir=image_dir, transform=transform)\nval_dataset = ImageDataset(dataframe=val_df, image_dir=image_dir, transform=transform)\n\n# Create the DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nprint('train is done')\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\nprint('val is done')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:22:37.393336Z","iopub.execute_input":"2024-06-09T20:22:37.394208Z","iopub.status.idle":"2024-06-09T20:22:37.880423Z","shell.execute_reply.started":"2024-06-09T20:22:37.394172Z","shell.execute_reply":"2024-06-09T20:22:37.879507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training parameters\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nage_predictor.to(device)\ncriterion = nn.MSELoss()  # Mean Squared Error loss\noptimizer = optim.Adam(age_predictor.parameters(), lr=0.001)\n\n# Lists to store the losses\ntrain_losses = []\nval_losses = []\n\n# Training loop\nnum_epochs = 20\nfor epoch in tqdm.tqdm(range(num_epochs)):\n    age_predictor.train()\n    train_loss = 0.0\n    print('here1')\n    for images, ages, genders in train_loader:\n        images, ages, genders = images.to(device), ages.to(device).float().unsqueeze(1), genders.to(device).float()\n        \n        optimizer.zero_grad()\n        outputs = age_predictor(images, genders)\n        loss = criterion(outputs, ages)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * images.size(0)\n    \n    train_loss /= len(train_loader.dataset)\n    train_losses.append(train_loss)\n    \n    # Validation loop\n    age_predictor.eval()\n    val_loss = 0.0\n    print('here2')\n    with torch.no_grad():\n        for images, ages, genders in val_loader:\n            images, ages, genders = images.to(device), ages.to(device).float().unsqueeze(1), genders.to(device).float()\n            \n            outputs = age_predictor(images, genders)\n            loss = criterion(outputs, ages)\n            \n            val_loss += loss.item() * images.size(0)\n    \n    val_loss /= len(val_loader.dataset)\n    val_losses.append(val_loss)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n\nprint(\"Training complete.\")\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:23:22.786313Z","iopub.execute_input":"2024-06-09T20:23:22.787062Z","iopub.status.idle":"2024-06-09T20:58:34.776147Z","shell.execute_reply.started":"2024-06-09T20:23:22.787025Z","shell.execute_reply":"2024-06-09T20:58:34.775223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:59:42.097686Z","iopub.execute_input":"2024-06-09T20:59:42.098480Z","iopub.status.idle":"2024-06-09T20:59:42.115183Z","shell.execute_reply.started":"2024-06-09T20:59:42.098446Z","shell.execute_reply":"2024-06-09T20:59:42.114270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom PIL import Image\nimport os\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport torch.nn.functional as F\ndef change_gender(g):\n    if g=='M':\n        return 1\n    else:\n        return 0\n# Load the test data\ntest_df = pd.read_csv(\"/kaggle/input/rsna-bone-age/boneage-test-dataset.csv\")\ntest_df['Image_Path'] = test_df['Case ID'].astype(str) + \".png\"\ntest_df['male'] = test_df['Sex'].apply(change_gender)\n\n# Define the test dataset and dataloader\nclass TestImageDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, image_dir, transform=None):\n        self.dataframe = dataframe\n        self.image_dir = image_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_name = self.dataframe.iloc[idx]['Image_Path']\n        gender = self.dataframe.iloc[idx]['male']\n        img_path = os.path.join(self.image_dir, img_name)\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, gender, img_name\n\n# Define the transformations\nclass SelfNormalize(object):\n    def __call__(self, img):\n        img_tensor = transforms.functional.to_tensor(img)\n        mean = img_tensor.mean(dim=(1, 2), keepdim=True)\n        std = img_tensor.std(dim=(1, 2), keepdim=True) + 1e-6\n        img_tensor = (img_tensor - mean) / std\n        return img_tensor\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    SelfNormalize()\n])\n\n# Create the test dataset and dataloader\ntest_image_dir = \"/kaggle/input/rsna-bone-age/boneage-test-dataset/boneage-test-dataset/\"\ntest_dataset = TestImageDataset(dataframe=test_df, image_dir=test_image_dir, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Load the trained model (assuming you have already trained the model and saved the state_dict)\n# age_predictor.load_state_dict(torch.load('path_to_saved_model.pth'))\nage_predictor.eval()\n\n# Move the model to the appropriate device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nage_predictor.to(device)\n\n# Make predictions\npredictions = []\nwith torch.no_grad():\n    for images, genders, img_names in test_loader:\n        images, genders = images.to(device), genders.to(device).float()\n        outputs = age_predictor(images, genders)\n        outputs = outputs.squeeze(1).cpu().numpy()\n        for img_name, pred in zip(img_names, outputs):\n            predictions.append((img_name, pred))\n\n# Convert predictions to a DataFrame\npred_df = pd.DataFrame(predictions, columns=['Image_Path', 'Predicted_Age'])\n\n# Save the DataFrame to a CSV file\n#pred_df.to_csv('predictions.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:01:27.344566Z","iopub.execute_input":"2024-06-09T21:01:27.345018Z","iopub.status.idle":"2024-06-09T21:01:39.211472Z","shell.execute_reply.started":"2024-06-09T21:01:27.344983Z","shell.execute_reply":"2024-06-09T21:01:39.210635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def id_gen(s):\n    return s[:4]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:03:09.661097Z","iopub.execute_input":"2024-06-09T21:03:09.661481Z","iopub.status.idle":"2024-06-09T21:03:09.665865Z","shell.execute_reply.started":"2024-06-09T21:03:09.661450Z","shell.execute_reply":"2024-06-09T21:03:09.664878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_gen('4360.png')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:03:23.132540Z","iopub.execute_input":"2024-06-09T21:03:23.133453Z","iopub.status.idle":"2024-06-09T21:03:23.139771Z","shell.execute_reply.started":"2024-06-09T21:03:23.133414Z","shell.execute_reply":"2024-06-09T21:03:23.138776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df['id']=pred_df['Image_Path'].apply(id_gen)\npred_df['pred']=pred_df['Predicted_Age']\npred_df=pred_df[['id','pred']]\npred_df.to_csv('pred.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:05:01.969993Z","iopub.execute_input":"2024-06-09T21:05:01.970843Z","iopub.status.idle":"2024-06-09T21:05:01.981404Z","shell.execute_reply.started":"2024-06-09T21:05:01.970811Z","shell.execute_reply":"2024-06-09T21:05:01.980286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = f'/kaggle/working/age_predictor_{epoch+1}.pth'\ntorch.save(age_predictor.state_dict(), model_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T17:45:58.206230Z","iopub.execute_input":"2024-06-09T17:45:58.206630Z","iopub.status.idle":"2024-06-09T17:45:59.124222Z","shell.execute_reply.started":"2024-06-09T17:45:58.206599Z","shell.execute_reply":"2024-06-09T17:45:59.123409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model file from the Kaggle dataset\nmodel_path = '/kaggle/input/w-net-pretrained/model_epoch_20.pth'  # Update the path to your dataset\n\n# Initialize the model\nmodel = WNet(5)  # Update the values as needed\n# Load the state dict\nmodel.load_state_dict(torch.load(model_path))\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T19:35:39.772764Z","iopub.execute_input":"2024-06-09T19:35:39.773116Z","iopub.status.idle":"2024-06-09T19:35:40.103898Z","shell.execute_reply.started":"2024-06-09T19:35:39.773088Z","shell.execute_reply":"2024-06-09T19:35:40.102690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(train_losses)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T17:46:48.137848Z","iopub.execute_input":"2024-06-09T17:46:48.138644Z","iopub.status.idle":"2024-06-09T17:46:48.440997Z","shell.execute_reply.started":"2024-06-09T17:46:48.138603Z","shell.execute_reply":"2024-06-09T17:46:48.439610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(val_losses)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T17:46:52.033759Z","iopub.execute_input":"2024-06-09T17:46:52.034164Z","iopub.status.idle":"2024-06-09T17:46:52.303887Z","shell.execute_reply.started":"2024-06-09T17:46:52.034132Z","shell.execute_reply":"2024-06-09T17:46:52.302696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model file from the Kaggle dataset\nmodel_path = '/kaggle/input/w-net-pretrained/model_epoch_20.pth'  # Update the path to your dataset\n\n# Assuming UEnc is already defined\nuenc = model.UEnc\ninput_dim = 16 * 64 * (224 // 16) * (224 // 16)  # Adjust based on the actual input dimensions\nhidden_dims = [512, 256, 128, 64, 32, 16]\noutput_dim = 1\n\nage_predictor = AgePredictor(uenc, input_dim, hidden_dims, output_dim)\n# Load the state dict\nage_predictor.load_state_dict(torch.load(model_path))\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nage_predictor.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res=[]\nages_l=[]\nwith torch.no_grad():\n    for images, ages, genders in tval_loader:\n        images, ages, genders = images.to(device), ages.to(device).float().unsqueeze(1), genders.to(device).float()\n            \n        outputs = age_predictor(images, genders)\n        for i in outputs:\n            res.append(i)\n        for i in ages:\n            ages_l.append(i)\n            \n        erors.append(eror)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T11:12:46.952118Z","iopub.execute_input":"2024-06-07T11:12:46.952941Z","iopub.status.idle":"2024-06-07T11:13:11.619773Z","shell.execute_reply.started":"2024-06-07T11:12:46.952910Z","shell.execute_reply":"2024-06-07T11:13:11.618899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.mean(erors))","metadata":{"execution":{"iopub.status.busy":"2024-06-07T11:11:17.888596Z","iopub.execute_input":"2024-06-07T11:11:17.888947Z","iopub.status.idle":"2024-06-07T11:11:17.921251Z","shell.execute_reply.started":"2024-06-07T11:11:17.888917Z","shell.execute_reply":"2024-06-07T11:11:17.920087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_train_data = pd.read_csv(\"/kaggle/input/rsna-bone-age/boneage-training-dataset.csv\")\nbone_age_train_data","metadata":{"execution":{"iopub.status.busy":"2024-06-04T11:38:09.838120Z","iopub.execute_input":"2024-06-04T11:38:09.838384Z","iopub.status.idle":"2024-06-04T11:38:09.877608Z","shell.execute_reply.started":"2024-06-04T11:38:09.838361Z","shell.execute_reply":"2024-06-04T11:38:09.876764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_train_data","metadata":{"execution":{"iopub.status.busy":"2024-06-07T09:36:05.313442Z","iopub.execute_input":"2024-06-07T09:36:05.314064Z","iopub.status.idle":"2024-06-07T09:36:05.330361Z","shell.execute_reply.started":"2024-06-07T09:36:05.314019Z","shell.execute_reply":"2024-06-07T09:36:05.329529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.max(bone_age_train_data['boneage'])/12","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:33:54.740286Z","iopub.execute_input":"2024-05-27T18:33:54.740619Z","iopub.status.idle":"2024-05-27T18:33:54.747845Z","shell.execute_reply.started":"2024-05-27T18:33:54.740594Z","shell.execute_reply":"2024-05-27T18:33:54.746787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_train_data['boneage']=bone_age_train_data['boneage']/12","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:34:01.233033Z","iopub.execute_input":"2024-05-27T18:34:01.233395Z","iopub.status.idle":"2024-05-27T18:34:01.238938Z","shell.execute_reply.started":"2024-05-27T18:34:01.233357Z","shell.execute_reply":"2024-05-27T18:34:01.237997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(bone_age_train_data['boneage'])","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:34:08.871651Z","iopub.execute_input":"2024-05-27T18:34:08.872037Z","iopub.status.idle":"2024-05-27T18:34:09.098961Z","shell.execute_reply.started":"2024-05-27T18:34:08.872008Z","shell.execute_reply":"2024-05-27T18:34:09.097978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_test_data = pd.read_csv(\"/kaggle/input/rsna-bone-age/boneage-test-dataset.csv\")\nbone_age_test_data","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:03:20.074929Z","iopub.execute_input":"2024-06-04T09:03:20.075313Z","iopub.status.idle":"2024-06-04T09:03:20.098742Z","shell.execute_reply.started":"2024-06-04T09:03:20.075285Z","shell.execute_reply":"2024-06-04T09:03:20.097595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = \"/kaggle/input/rsna-bone-age/boneage-training-dataset/boneage-training-dataset\"\ntest_path = \"/kaggle/input/rsna-bone-age/boneage-test-dataset/boneage-test-dataset\"\n\ntrain_folder = os.listdir(train_path)\ntest_folder = os.listdir(test_path)\n\nprint(len(train_folder))\nprint(len(test_folder))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T11:46:43.166733Z","iopub.execute_input":"2024-06-04T11:46:43.167515Z","iopub.status.idle":"2024-06-04T11:46:43.198353Z","shell.execute_reply.started":"2024-06-04T11:46:43.167480Z","shell.execute_reply":"2024-06-04T11:46:43.197295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_folder[:10]","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:02:36.936013Z","iopub.execute_input":"2024-06-04T09:02:36.936413Z","iopub.status.idle":"2024-06-04T09:02:36.943310Z","shell.execute_reply.started":"2024-06-04T09:02:36.936381Z","shell.execute_reply":"2024-06-04T09:02:36.942175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_train_data[bone_age_train_data['id']==9273]","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:02:39.094950Z","iopub.execute_input":"2024-06-04T09:02:39.095368Z","iopub.status.idle":"2024-06-04T09:02:39.112239Z","shell.execute_reply.started":"2024-06-04T09:02:39.095338Z","shell.execute_reply":"2024-06-04T09:02:39.111010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter=1\nfor i in ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_train_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:31:41.805460Z","iopub.execute_input":"2024-05-27T18:31:41.806057Z","iopub.status.idle":"2024-05-27T18:31:41.816513Z","shell.execute_reply.started":"2024-05-27T18:31:41.806023Z","shell.execute_reply":"2024-05-27T18:31:41.815573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_list_id = bone_age_train_data['id'].to_list()\ntest_list_id = bone_age_test_data['Case ID'].to_list()","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:03:25.374600Z","iopub.execute_input":"2024-06-04T09:03:25.374979Z","iopub.status.idle":"2024-06-04T09:03:25.381307Z","shell.execute_reply.started":"2024-06-04T09:03:25.374951Z","shell.execute_reply":"2024-06-04T09:03:25.379978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_folder[:10]","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:03:28.171026Z","iopub.execute_input":"2024-06-04T09:03:28.171735Z","iopub.status.idle":"2024-06-04T09:03:28.178604Z","shell.execute_reply.started":"2024-06-04T09:03:28.171702Z","shell.execute_reply":"2024-06-04T09:03:28.177467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_train_data['Image_Path'] = [f\"{str(i)}.png\" for i in train_list_id]\nbone_age_test_data['Image Path'] = [f\"{str(i)}.png\" for i in test_list_id]","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:03:30.835091Z","iopub.execute_input":"2024-06-04T09:03:30.835740Z","iopub.status.idle":"2024-06-04T09:03:30.848107Z","shell.execute_reply.started":"2024-06-04T09:03:30.835710Z","shell.execute_reply":"2024-06-04T09:03:30.847140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_train_data","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:03:42.336919Z","iopub.execute_input":"2024-06-04T09:03:42.337467Z","iopub.status.idle":"2024-06-04T09:03:42.354071Z","shell.execute_reply.started":"2024-06-04T09:03:42.337407Z","shell.execute_reply":"2024-06-04T09:03:42.352761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_int(i):\n    return int(i)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:34:53.539327Z","iopub.execute_input":"2024-05-27T18:34:53.540047Z","iopub.status.idle":"2024-05-27T18:34:53.544056Z","shell.execute_reply.started":"2024-05-27T18:34:53.540011Z","shell.execute_reply":"2024-05-27T18:34:53.543090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_train_data['boneage']=bone_age_train_data['boneage'].apply(to_int)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:35:11.883116Z","iopub.execute_input":"2024-05-27T18:35:11.883768Z","iopub.status.idle":"2024-05-27T18:35:11.897880Z","shell.execute_reply.started":"2024-05-27T18:35:11.883734Z","shell.execute_reply":"2024-05-27T18:35:11.896840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_test_data","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:33:35.324538Z","iopub.execute_input":"2024-05-27T18:33:35.325159Z","iopub.status.idle":"2024-05-27T18:33:35.337482Z","shell.execute_reply.started":"2024-05-27T18:33:35.325122Z","shell.execute_reply":"2024-05-27T18:33:35.336490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_test_data.columns = ['id','Sex','Image Path']","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:35:11.200890Z","iopub.execute_input":"2024-05-26T16:35:11.201749Z","iopub.status.idle":"2024-05-26T16:35:11.206457Z","shell.execute_reply.started":"2024-05-26T16:35:11.201715Z","shell.execute_reply":"2024-05-26T16:35:11.205368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_train_data.columns = ['id','boneage', 'Sex', 'Image']","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:35:18.531688Z","iopub.execute_input":"2024-05-27T18:35:18.532550Z","iopub.status.idle":"2024-05-27T18:35:18.536908Z","shell.execute_reply.started":"2024-05-27T18:35:18.532516Z","shell.execute_reply":"2024-05-27T18:35:18.535782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_train_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:35:19.803341Z","iopub.execute_input":"2024-05-27T18:35:19.803670Z","iopub.status.idle":"2024-05-27T18:35:19.814136Z","shell.execute_reply.started":"2024-05-27T18:35:19.803644Z","shell.execute_reply":"2024-05-27T18:35:19.813186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_test_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:35:25.683124Z","iopub.execute_input":"2024-05-27T18:35:25.683995Z","iopub.status.idle":"2024-05-27T18:35:25.693541Z","shell.execute_reply.started":"2024-05-27T18:35:25.683958Z","shell.execute_reply":"2024-05-27T18:35:25.692523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_train_data['Sex'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:35:28.579197Z","iopub.execute_input":"2024-05-27T18:35:28.580023Z","iopub.status.idle":"2024-05-27T18:35:28.593299Z","shell.execute_reply.started":"2024-05-27T18:35:28.579991Z","shell.execute_reply":"2024-05-27T18:35:28.592276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_test_data['Sex'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:35:31.170080Z","iopub.execute_input":"2024-05-27T18:35:31.170667Z","iopub.status.idle":"2024-05-27T18:35:31.180636Z","shell.execute_reply.started":"2024-05-27T18:35:31.170632Z","shell.execute_reply":"2024-05-27T18:35:31.179775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# True - Male  or  M - 1        \n# False - Female or F - 0\n\nbone_age_train_data['Sex'] = bone_age_train_data['Sex'].replace({'False':0,'True':1})\nbone_age_test_data['Sex'] = bone_age_test_data['Sex'].replace({'F':0,'M':1})","metadata":{"execution":{"iopub.status.busy":"2024-05-27T18:35:35.352998Z","iopub.execute_input":"2024-05-27T18:35:35.353366Z","iopub.status.idle":"2024-05-27T18:35:35.359722Z","shell.execute_reply.started":"2024-05-27T18:35:35.353334Z","shell.execute_reply":"2024-05-27T18:35:35.358750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_train_data['Sex']=bone_age_train_data['Sex'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:35:37.684775Z","iopub.execute_input":"2024-05-26T16:35:37.685744Z","iopub.status.idle":"2024-05-26T16:35:37.690327Z","shell.execute_reply.started":"2024-05-26T16:35:37.685706Z","shell.execute_reply":"2024-05-26T16:35:37.689610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_train_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:35:40.855072Z","iopub.execute_input":"2024-05-26T16:35:40.855856Z","iopub.status.idle":"2024-05-26T16:35:40.866870Z","shell.execute_reply.started":"2024-05-26T16:35:40.855816Z","shell.execute_reply":"2024-05-26T16:35:40.865799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bone_age_test_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:35:44.861207Z","iopub.execute_input":"2024-05-26T16:35:44.861583Z","iopub.status.idle":"2024-05-26T16:35:44.872620Z","shell.execute_reply.started":"2024-05-26T16:35:44.861553Z","shell.execute_reply":"2024-05-26T16:35:44.871572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image=Image.open(\"/kaggle/input/rsna-bone-age/boneage-training-dataset/boneage-training-dataset/\"+'1377.png')\n#image = image.resize((224,224))\n#image = image.convert(\"BW\")\nimage = np.array(image)\nplt.imshow(image,cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2024-06-04T11:38:09.891030Z","iopub.execute_input":"2024-06-04T11:38:09.892928Z","iopub.status.idle":"2024-06-04T11:38:10.540036Z","shell.execute_reply.started":"2024-06-04T11:38:09.892901Z","shell.execute_reply":"2024-06-04T11:38:10.539081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(images[0][2,:,:],cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:38:37.033356Z","iopub.execute_input":"2024-06-04T09:38:37.034040Z","iopub.status.idle":"2024-06-04T09:38:37.371117Z","shell.execute_reply.started":"2024-06-04T09:38:37.033997Z","shell.execute_reply":"2024-06-04T09:38:37.370140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for img in bone_age_train_data['Image Path'][:5]:\n    image = Image.open(\"/kaggle/input/rsna-bone-age/boneage-training-dataset/boneage-training-dataset/\"+img)\n    #(image = image.resize((224,224))\n    #image = image.convert(\"RGB\")\n    image = np.array(image)\n# Plot a histogram of the distribution of the pixels\n    sns.distplot(image.flatten(), \n             label=f'Pixel Mean {np.mean(image.flatten()):.4f} & Standard Deviation {np.std(image.flatten()):.4f}', kde=False)\n    plt.legend(loc='upper center')\n    plt.title('Distribution of Pixel Intensities in the Image')\n    plt.xlabel('Pixel Intensity')\n    plt.ylabel('# Pixels in Image')\n    plt.show()\n    #train_data.append(image)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:49:20.365356Z","iopub.execute_input":"2024-05-26T16:49:20.366215Z","iopub.status.idle":"2024-05-26T16:49:22.955940Z","shell.execute_reply.started":"2024-05-26T16:49:20.366181Z","shell.execute_reply":"2024-05-26T16:49:22.954884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style(\"white\")\ngenerated_image, label = generator.__getitem__(0)\nplt.imshow(generated_image[0], cmap='gray')\nplt.colorbar()\nplt.title('Raw Chest X Ray Image')\nprint(f\"The dimensions of the image are {generated_image.shape[1]} pixels width and {generated_image.shape[2]} pixels height\")\nprint(f\"The maximum pixel value is {generated_image.max():.4f} and the minimum is {generated_image.min():.4f}\")\nprint(f\"The mean value of the pixels is {generated_image.mean():.4f} and the standard deviation is {generated_image.std():.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T17:08:06.641259Z","iopub.execute_input":"2024-05-26T17:08:06.642057Z","iopub.status.idle":"2024-05-26T17:08:07.305551Z","shell.execute_reply.started":"2024-05-26T17:08:06.642021Z","shell.execute_reply":"2024-05-26T17:08:07.304582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_image[0][:,:,2]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T17:18:47.716331Z","iopub.execute_input":"2024-05-26T17:18:47.717213Z","iopub.status.idle":"2024-05-26T17:18:47.725013Z","shell.execute_reply.started":"2024-05-26T17:18:47.717170Z","shell.execute_reply":"2024-05-26T17:18:47.724180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Include a histogram of the distribution of the pixels\nimage=Image.open(\"/kaggle/input/rsna-bone-age/boneage-training-dataset/boneage-training-dataset/\"+'1377.png')\n#image = image.resize((224,224))\n#image = image.convert(\"BW\")\nimage = np.array(image)\nsns.set()\nplt.figure(figsize=(10, 7))\n\n# Plot histogram for original iamge\n#sns.distplot(image.flatten(), \n #            label=f'Original Image: mean {np.mean(image.flatten())} - Standard Deviation {np.std(image.flatten())} \\n '\n  #           f'Min pixel value {np.min(image.flatten())} - Max pixel value {np.max(image.flatten())}',\n    #         color='blue', \n     #        kde=False)\n\n# Plot histogram for generated image\nsns.distplot(generated_image[0].ravel(), \n             label=f'Generated Image: mean {np.mean(generated_image[0]):.4f} - Standard Deviation {np.std(generated_image[0]):.4f} \\n'\n             f'Min pixel value {np.min(generated_image[0]):.4} - Max pixel value {np.max(generated_image[0]):.4}', \n             color='red', \n             kde=False)\n\n# Place legends\nplt.legend()\nplt.title('Distribution of Pixel Intensities in the Image')\nplt.xlabel('Pixel Intensity')\nplt.ylabel('# Pixel')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T17:10:47.543402Z","iopub.execute_input":"2024-05-26T17:10:47.543830Z","iopub.status.idle":"2024-05-26T17:10:48.136612Z","shell.execute_reply.started":"2024-05-26T17:10:47.543798Z","shell.execute_reply":"2024-05-26T17:10:48.135640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-05-26T17:20:31.199475Z","iopub.execute_input":"2024-05-26T17:20:31.199871Z","iopub.status.idle":"2024-05-26T17:20:31.209611Z","shell.execute_reply.started":"2024-05-26T17:20:31.199839Z","shell.execute_reply":"2024-05-26T17:20:31.208560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l=[]\nbins=np.arange(-3,3,0.015)\nfor i in tqdm.tqdm(range(10000)):\n    generated_image, label = generator.__getitem__(i)\n    \n    l.append(np.histogram(generated_image[0].ravel(), bins=bins)[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:01:35.642259Z","iopub.execute_input":"2024-05-26T18:01:35.642689Z","iopub.status.idle":"2024-05-26T18:09:59.148763Z","shell.execute_reply.started":"2024-05-26T18:01:35.642653Z","shell.execute_reply":"2024-05-26T18:09:59.147632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l=np.array(l)\nplt.plot(np.mean(l,axis=0))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:38:42.134236Z","iopub.execute_input":"2024-05-26T18:38:42.134577Z","iopub.status.idle":"2024-05-26T18:38:42.475178Z","shell.execute_reply.started":"2024-05-26T18:38:42.134551Z","shell.execute_reply":"2024-05-26T18:38:42.474121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(new_im,cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:40:52.268201Z","iopub.execute_input":"2024-05-26T16:40:52.269086Z","iopub.status.idle":"2024-05-26T16:40:52.737562Z","shell.execute_reply.started":"2024-05-26T16:40:52.269044Z","shell.execute_reply":"2024-05-26T16:40:52.736678Z"},"trusted":true},"execution_count":null,"outputs":[]}]}